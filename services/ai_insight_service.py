from datetime import datetime, timedelta
from database import DatabasePool
from services.ai_service import ai_service
import json
import logging

logger = logging.getLogger(__name__)


class AIInsightService:
    @staticmethod
    def generate_daily_advice(project_id, force_refresh=False):
        """èšåˆæ—¥æŠ¥ã€ä»»åŠ¡å’Œè¿›åº¦ï¼Œç”ŸæˆAIæ¯æ—¥å»ºè®®ï¼ˆæ”¯æŒå½“æ—¥ç¼“å­˜ï¼‰"""
        try:
            today_date = datetime.now().strftime('%Y-%m-%d')
            
            # 1. æ£€æŸ¥ç¼“å­˜
            if not force_refresh:
                with DatabasePool.get_connection() as conn:
                    cache = conn.execute('''
                        SELECT content FROM ai_report_cache 
                        WHERE project_id = ? AND report_type = 'daily_advice'
                        AND date(created_at) = ?
                    ''', (project_id, today_date)).fetchone()
                    if cache:
                        return cache['content']

            with DatabasePool.get_connection() as conn:
                # 2. è·å–é¡¹ç›®åŸºæœ¬ä¿¡æ¯
                project = conn.execute('SELECT id, project_name, progress, status FROM projects WHERE id = ?', (project_id,)).fetchone()
                if not project:
                    return "é¡¹ç›®ä¸å­˜åœ¨"

                # 3. è·å–æœ€è¿‘ 3 å¤©çš„æ—¥æŠ¥
                three_days_ago = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
                reports = conn.execute('''
                    SELECT log_date, work_content, issues_encountered 
                    FROM work_logs 
                    WHERE project_id = ? AND log_date >= ?
                    ORDER BY log_date DESC
                ''', (project_id, three_days_ago)).fetchall()
                
                report_text = "\n".join([
                    f"[{r['log_date']}] å·¥ä½œ: {r['work_content']} | é—®é¢˜: {r['issues_encountered']}" 
                    for r in reports
                ]) if reports else "æœ€è¿‘æ— æ—¥æŠ¥è®°å½•"

                # 4. è·å–è¿‘æœŸæœªå®Œæˆçš„é«˜ä¼˜å…ˆçº§ä»»åŠ¡
                tasks = conn.execute('''
                    SELECT t.task_name, s.stage_name 
                    FROM tasks t
                    JOIN project_stages s ON t.stage_id = s.id
                    WHERE s.project_id = ? AND t.is_completed = 0
                    LIMIT 10
                ''', (project_id,)).fetchall()
                
                task_text = "\n".join([f"- {t['stage_name']}: {t['task_name']}" for t in tasks]) if tasks else "å½“å‰æ— å¾…åŠä»»åŠ¡"

                # 5. æ„é€  Prompt
                system_prompt = """ä½ æ˜¯ä¸€åä¸–ç•Œé¡¶çº§ ICU åŒ»ç–—ä¿¡æ¯åŒ–é¡¹ç›®æ€»ç›‘ (Project Director)ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®é¡¹ç›®æ•°æ®è¿›è¡Œæ·±åº¦ç©¿é€åˆ†æã€‚ä¸¥ç¦ä½¿ç”¨â€œæ‚¨å¥½â€ã€â€œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨â€ç­‰ä»»ä½•å®¢å¥—è¯ã€‚
ä½ çš„å›å¤å¿…é¡»ç›´æ¥ã€ä¸“ä¸šã€æç®€ï¼Œåƒæ˜¯ä¸€ä»½å‘ˆé€ç»™é«˜ç®¡çš„ç´§æ€¥ç®€æŠ¥ã€‚

æ ¼å¼è¦æ±‚ (ä¸¥æ ¼æ‰§è¡Œ)ï¼š
1. ğŸ¯ **ç°çŠ¶å®šæ€§**ï¼šä¸€å¥è¯è¯´æ˜é¡¹ç›®å½“å‰çš„æ ¸å¿ƒåŸºè°ƒã€‚
2. ğŸš© **çº¢çº¿é¢„è­¦**ï¼šä»…åˆ—å‡ºæœ€è‡´å‘½çš„1ä¸ªé£é™©ã€‚
3. âš¡ **å½“æ—¥å¿…åŠ**ï¼šç»™å‡º3æ¡â€œå¦‚æœä¸åšå°±ä¼šå¯¼è‡´å»¶æœŸâ€çš„æå…¶å…·ä½“çš„æŒ‡ä»¤ã€‚
æ³¨æ„ï¼šç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦ä»»ä½•å‰ç¼€æˆ–åç¼€ã€‚"""

                user_content = f"""ä½ æ˜¯ä¸€åä¸–ç•Œé¡¶çº§ ICU åŒ»ç–—ä¿¡æ¯åŒ–é¡¹ç›®æ€»ç›‘ã€‚è¯·æ ¹æ®ä»¥ä¸‹é¡¹ç›®æ•°æ®è¿›è¡Œæ·±åº¦ç©¿é€åˆ†æã€‚
ä¸¥ç¦ä½¿ç”¨"æ‚¨å¥½"ã€"æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨"ç­‰å®¢å¥—è¯ï¼Œä¸¥ç¦è¯¢é—®ç”¨æˆ·éœ€æ±‚ï¼Œç›´æ¥è¾“å‡ºåˆ†æç»“æœã€‚

æ ¼å¼è¦æ±‚ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ï¼š
1. ğŸ¯ **ç°çŠ¶å®šæ€§**ï¼šä¸€å¥è¯è¯´æ˜é¡¹ç›®å½“å‰çš„æ ¸å¿ƒåŸºè°ƒã€‚
2. ğŸš© **çº¢çº¿é¢„è­¦**ï¼šä»…åˆ—å‡ºæœ€è‡´å‘½çš„1ä¸ªé£é™©ã€‚
3. âš¡ **å½“æ—¥å¿…åŠ**ï¼šç»™å‡º3æ¡"å¦‚æœä¸åšå°±ä¼šå¯¼è‡´å»¶æœŸ"çš„æå…¶å…·ä½“çš„æŒ‡ä»¤ã€‚

é¡¹ç›®åç§°: {project['project_name']}
å½“å‰çŠ¶æ€: {project['status']} (è¿›åº¦: {project['progress']}%)

ã€è¿‘æœŸæ—¥æŠ¥æ‘˜è¦ã€‘
{report_text}

ã€å¾…åŠä»»åŠ¡æ¸…å•ã€‘
{task_text}

è¯·ç›´æ¥è¾“å‡ºåˆ†æç»“æœï¼Œä¸è¦ä»»ä½•å‰ç¼€æˆ–åç¼€ï¼š"""

                # 6. è°ƒç”¨ AI
                advice = ai_service.call_ai_api(system_prompt, user_content, task_type="analysis")
                
                if advice:
                    # 7. æ›´æ–°ç¼“å­˜
                    with DatabasePool.get_connection() as conn:
                        # å…ˆåˆ é™¤ä»Šæ—¥æ—§ç¼“å­˜
                        conn.execute('''
                            DELETE FROM ai_report_cache 
                            WHERE project_id = ? AND report_type = 'daily_advice'
                            AND date(created_at) = ?
                        ''', (project_id, today_date))
                        
                        res = conn.execute('''
                            INSERT INTO ai_report_cache (project_id, report_type, content, created_at)
                            VALUES (?, ?, ?, ?)
                        ''', (project_id, 'daily_advice', advice, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
                        conn.commit()
                        print(f"[DEBUG] Cache saved for project {project_id}, rowcount: {res.rowcount}")
                
                return advice or "AI æš‚æ—¶æ— æ³•ç”Ÿæˆå»ºè®®ï¼Œè¯·æ ¸æŸ¥ç½‘ç»œæˆ–é…ç½®ã€‚"
        except Exception as e:
            print(f"Generate Daily Advice Error: {e}")
            return f"ç”Ÿæˆ AI å»ºè®®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


    @staticmethod
    def analyze_trends(project_id):
        """åˆ†æé¡¹ç›®é£é™©è¶‹åŠ¿ã€é€Ÿåº¦å’Œé—®é¢˜å¯†åº¦"""
        try:
            with DatabasePool.get_connection() as conn:
                # 1. è·å–è¿‘30å¤©çš„é£é™©è¯„åˆ†å†å²
                history = conn.execute('''
                    SELECT record_date, risk_score, sentiment_score 
                    FROM project_risk_history 
                    WHERE project_id = ? 
                    ORDER BY record_date ASC
                    LIMIT 30
                ''', (project_id,)).fetchall()
                
                dates = [h['record_date'] for h in history]
                risk_scores = [h['risk_score'] for h in history]
                sentiment_scores = [h['sentiment_score'] for h in history]

                # 2. è®¡ç®—Velocity (æ¯å‘¨å®Œæˆä»»åŠ¡æ•°) - è¿‘4å‘¨
                velocity_data = []
                for i in range(4):
                    start_date = (datetime.now() - timedelta(weeks=i+1)).strftime('%Y-%m-%d')
                    end_date = (datetime.now() - timedelta(weeks=i)).strftime('%Y-%m-%d')
                    count = conn.execute('''
                        SELECT COUNT(*) as completed_count
                        FROM tasks t
                        JOIN project_stages s ON t.stage_id = s.id
                        WHERE s.project_id = ? AND t.is_completed = 1 
                        AND t.completed_date >= ? AND t.completed_date < ?
                    ''', (project_id, start_date, end_date)).fetchone()['completed_count']
                    velocity_data.append({'week_start': start_date, 'count': count})
                velocity_data.reverse() # æŒ‰æ—¶é—´æ­£åº

                # 3. è®¡ç®—é—®é¢˜å¯†åº¦è¶‹åŠ¿ (æ´»è·ƒé—®é¢˜æ•°) - ç®€å•é‡‡æ ·
                issue_trend = []
                # æ³¨æ„ï¼šç”±äºæ²¡æœ‰é—®é¢˜å†å²å¿«ç…§è¡¨ï¼Œè¿™é‡Œæš‚æ—¶åªèƒ½è¿”å›å½“å‰é—®é¢˜çŠ¶æ€ï¼Œæˆ–è€…åŸºäº issues è¡¨çš„ created_at/resolved_at å€’æ¨
                # è¿™é‡Œé‡‡ç”¨ç®€åŒ–æ–¹æ¡ˆï¼šæŒ‰å‘¨ç»Ÿè®¡â€œæ–°å¢é—®é¢˜â€å’Œâ€œè§£å†³é—®é¢˜â€
                for i in range(4):
                    start_date = (datetime.now() - timedelta(weeks=i+1)).strftime('%Y-%m-%d')
                    end_date = (datetime.now() - timedelta(weeks=i)).strftime('%Y-%m-%d')
                    created = conn.execute('SELECT COUNT(*) as c FROM issues WHERE project_id=? AND created_at >= ? AND created_at < ?', (project_id, start_date, end_date)).fetchone()['c']
                    resolved = conn.execute('SELECT COUNT(*) as c FROM issues WHERE project_id=? AND resolved_at >= ? AND resolved_at < ?', (project_id, start_date, end_date)).fetchone()['c']
                    issue_trend.append({'week_start': start_date, 'created': created, 'resolved': resolved})
                issue_trend.reverse()

                return {
                    'dates': dates,
                    'risk_scores': risk_scores,
                    'sentiment_scores': sentiment_scores,
                    'velocity': velocity_data,
                    'issue_trend': issue_trend
                }
        except Exception as e:
            print(f"Error analyzing trends: {e}")
            return {'error': str(e)}


    @staticmethod
    def analyze_sentiment(project_id):
        """åˆ†æé¡¹ç›®æƒ…æ„Ÿå€¾å‘ä¸å››ç»´åº¦è¯„åˆ†"""
        try:
            with DatabasePool.get_connection() as conn:
                # è·å–è¿‘7å¤©æ—¥æŠ¥ä¸æœªè§£å†³çš„é«˜ä¼˜å…ˆçº§é—®é¢˜
                seven_days_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
                logs = conn.execute("SELECT work_content, issues_encountered FROM work_logs WHERE project_id=? AND log_date >= ?", (project_id, seven_days_ago)).fetchall()
                # issuesè¡¨æ²¡æœ‰ title/priority, ä½¿ç”¨ issue_type/description/severity
                issues = conn.execute("SELECT issue_type, description, severity FROM issues WHERE project_id=? AND status != 'å·²è§£å†³'", (project_id,)).fetchall()
                
                text_corpus = "\n".join([f"Log: {l['work_content']} {l['issues_encountered']}" for l in logs])
                text_corpus += "\n".join([f"Issue: [{i['issue_type']}] {i['description']} (Severity: {i['severity']})" for i in issues])
                
                if not text_corpus.strip():
                    return {'scores': {'client': 8, 'team': 8, 'tech': 8, 'progress': 8}, 'signals': []}

                system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®é£é™©åˆ†æå¼•æ“ã€‚è¯·åŸºäºé¡¹ç›®æ—¥å¿—å’Œå½“å‰é—®é¢˜ï¼Œå¯¹é¡¹ç›®çš„å¥åº·åº¦è¿›è¡Œå¤šç»´åº¦è¯„ä¼° (0-10åˆ†ï¼Œ10æœ€å¥½)ã€‚

è¯„ä¼°ç»´åº¦ï¼š
1. å®¢æˆ·æ»¡æ„åº¦ (Client Satisfaction)
2. å›¢é˜Ÿå£«æ°” (Team Morale)
3. æŠ€æœ¯ç¨³å®šæ€§ (Technical Stability)
4. è¿›åº¦ä¿¡å¿ƒ (Progress Confidence)

ä¸¥é‡ç­‰çº§åˆ¤å®šè§„åˆ™ (Severity)ï¼š
- Critical: æ¶‰åŠâ€œåœæ­¢å±¥è¡ŒåˆåŒâ€ã€â€œä¸»åŠ¨é€€åœºâ€ã€â€œæ ¸å¿ƒå›¢é˜Ÿè§£æ•£â€ã€â€œé¡¹ç›®ä¸­æ­¢â€ç­‰è‡´å‘½é£é™©ã€‚
- High: å­˜åœ¨å¤šä¸ªé«˜ä¼˜å…ˆçº§ Bugï¼Œæˆ–å…³é”®è·¯å¾„é‡Œç¨‹ç¢‘å·²ä¸¥é‡å¤±æ§ã€‚
- Medium: è¿›åº¦æœ‰æ‰€æ»åï¼Œæˆ–å­˜åœ¨æ²Ÿé€šæ‘©æ“¦ã€‚
- Low: é£é™©å¯æ§ï¼Œä»…æœ‰çç¢é—®é¢˜ã€‚

è¯·ä¸¥æ ¼è¿”å›å¦‚ä¸‹æ ¼å¼çš„åˆæ³• JSONï¼š
{
    "scores": {
        "client": 8.5,
        "team": 7.0,
        "tech": 6.0,
        "progress": 5.0
    },
    "severity": "High/Critical/Medium/Low",
    "summary": "1-2å¥å¯¹å½“å‰å±€åŠ¿çš„ç²¾ç‚¼æ€»ç»“",
    "signals": ["ä¿¡å·1", "ä¿¡å·2", ...] 
}"""
                # è°ƒç”¨AIè¿›è¡Œåˆ†æ
                import json
                try:
                    ai_resp = ai_service.call_ai_api(system_prompt, text_corpus, task_type="analysis")
                    # å°è¯•æ¸…ç†markdownæ ‡è®°
                    if ai_resp.startswith('```json'):
                        ai_resp = ai_resp.replace('```json', '').replace('```', '')
                    result = json.loads(ai_resp)
                except Exception as ex:
                    # Fallback if AI fails or returns bad JSON
                    print(f"[Sentiment Error] JSON loads failed: {ex} on resp: {ai_resp}")
                    result = {
                        'scores': {'client': 7, 'team': 7, 'tech': 7, 'progress': 7}, 
                        'signals': ['AIè§£æå¤±è´¥']
                    }

                return result
        except Exception as e:
            print(f"Sentiment Analysis Error: {e}")
            return {'error': str(e)}

    @staticmethod
    def snapshot_project_risk(project_id):
        """(å®šæ—¶ä»»åŠ¡ç”¨) å¿«ç…§å½“å‰é¡¹ç›®çš„é£é™©ä¸æƒ…æ„ŸçŠ¶æ€è‡³å†å²è¡¨"""
        try:
            # 1. è®¡ç®—å„é¡¹æŒ‡æ ‡
            sentiment = AIInsightService.analyze_sentiment(project_id)
            if 'error' in sentiment: return False

            # è®¡ç®—å¹³å‡æƒ…æ„Ÿåˆ† (0-1å½’ä¸€åŒ–ï¼Œç”¨äºå­˜å…¥ sentiment_score)
            avg_sentiment = (sentiment['client'] + sentiment['team'] + sentiment['tech'] + sentiment['progress']) / 40.0
            
            # ç®€æ˜“è®¡ç®—é£é™©åˆ† (è¿™é‡Œå¤ç”¨ analyze_trends é‡Œçš„é€»è¾‘æˆ–è°ƒç”¨ risk_serviceï¼Œä¸ºç®€åŒ–ç›´æ¥æ¨¡æ‹Ÿ)
            # å®é™…åº”è°ƒç”¨ RiskService.assess_project_risk(project_id)
            current_risk_score = 30 + (1.0 - avg_sentiment) * 40 # æƒ…æ„Ÿè¶Šä½é£é™©è¶Šé«˜

            with DatabasePool.get_connection() as conn:
                conn.execute('''
                    INSERT INTO project_risk_history (project_id, record_date, risk_score, sentiment_score, trend_direction, key_risk_factors)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    project_id, 
                    datetime.now().strftime('%Y-%m-%d'), 
                    current_risk_score, 
                    avg_sentiment, 
                    'stable', 
                    ",".join(sentiment.get('signals', []))
                ))
            return True
        except Exception as e:
            print(f"Snapshot Error: {e}")
            return False

    @staticmethod
    def parse_work_log(raw_text):
        """è§£æéç»“æ„åŒ–æ–‡æœ¬ä¸ºç»“æ„åŒ–æ—¥æŠ¥"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®ç®¡ç†åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥çš„éç»“æ„åŒ–æ–‡æœ¬ï¼Œæå–å¹¶æ•´ç†ä¸ºä»¥ä¸‹JSONç»“æ„ï¼š
{
    "work_content": "ä»Šæ—¥å®Œæˆçš„å…·ä½“å·¥ä½œå†…å®¹",
    "issues_encountered": "é‡åˆ°çš„é—®é¢˜æˆ–å›°éš¾",
    "work_hours": 0.0,
    "tomorrow_plan": "æ˜æ—¥è®¡åˆ’"
}
è§„åˆ™ï¼š
1. work_hours æå–ä¸ºæµ®ç‚¹æ•°ï¼ˆå¦‚ "åŠå¤©"->4.0, "2å°æ—¶"->2.0ï¼‰ã€‚å¦‚æœæœªæåŠï¼Œé»˜è®¤ä¸º 8.0ã€‚
2. å¦‚æœæ²¡æœ‰æåˆ°é—®é¢˜ï¼Œissues_encountered å¡« "æ— "ã€‚
3. ä¿æŒè¯­æ°”ä¸“ä¸šã€‚"""
        
        import json
        try:
            ai_resp = ai_service.call_ai_api(system_prompt, raw_text, task_type="analysis")
            # æ¸…ç†å¯èƒ½çš„markdown
            if ai_resp.startswith('```json'):
                ai_resp = ai_resp.replace('```json', '').replace('```', '')
            
            data = json.loads(ai_resp)
            return data
        except Exception as e:
            print(f"AI Parse Log Error: {e}")
            # Fallback
            return {
                "work_content": raw_text,
                "issues_encountered": "",
                "work_hours": 8.0,
                "tomorrow_plan": ""
            }

    @staticmethod
    def get_stale_items(project_id):
        """è·å–é¡¹ç›®ä¸­çš„æ»å/è¿‡æœŸé¡¹ (é—®é¢˜ã€æ¥å£ã€é‡Œç¨‹ç¢‘)"""
        try:
            with DatabasePool.get_connection() as conn:
                stale_items = []
                now = datetime.now()
                seven_days_ago = now - timedelta(days=7)
                three_days_later = now + timedelta(days=3)

                # 1. æ»åé—®é¢˜ (æœªè§£å†³ä¸”åˆ›å»ºè¶…è¿‡7å¤©)
                issues = conn.execute('''
                    SELECT id, description, severity, status, created_at, 'issue' as type
                    FROM issues 
                    WHERE project_id = ? AND status != 'å·²è§£å†³'
                ''', (project_id,)).fetchall()
                
                for i in issues:
                    try:
                        # Handle potential space in TIMESTAMP or just DATE
                        created_at_str = i['created_at'].split(' ')[0] if i['created_at'] else ""
                        if not created_at_str: continue
                        created_at = datetime.strptime(created_at_str, '%Y-%m-%d')
                        if created_at < seven_days_ago:

                            item = dict(i)
                            item['title'] = f"[{i['severity']}] {i['description'][:20]}..."
                            item['reason'] = f"åˆ›å»ºäº {i['created_at']}ï¼Œå·²è¶…è¿‡7å¤©æœªè§£å†³"
                            stale_items.append(item)
                    except:
                        pass # Ignore parse errors

                # 2. æœªå®Œæˆæ¥å£ (ç®€å•é€»è¾‘ï¼šæ‰€æœ‰æœªå®Œæˆçš„)
                interfaces = conn.execute('''
                    SELECT id, system_name, interface_name, status, 'interface' as type
                    FROM interfaces 
                    WHERE project_id = ? AND status != 'å·²å®Œæˆ'
                ''', (project_id,)).fetchall()
                
                for i in interfaces:
                    item = dict(i)
                    item['title'] = f"{i['system_name']} - {i['interface_name']}"
                    item['reason'] = f"å½“å‰çŠ¶æ€: {i['status']}"
                    stale_items.append(item)

                # 3. ä¸´è¿‘æˆ–è¿‡æœŸé‡Œç¨‹ç¢‘ (æœªå®Œæˆä¸” target_date < now + 3 days)
                milestones = conn.execute('''
                    SELECT id, name, target_date, is_completed, 'milestone' as type
                    FROM milestones 
                    WHERE project_id = ? AND is_completed = 0
                ''', (project_id,)).fetchall()
                
                for m in milestones:
                    try:
                        target_date = datetime.strptime(m['target_date'], '%Y-%m-%d')
                        if target_date < three_days_later:
                            item = dict(m)
                            item['title'] = m['name']
                            days_diff = (target_date - now).days
                            if days_diff < 0:
                                item['reason'] = f"å·²é€¾æœŸ {abs(days_diff)} å¤© ({m['target_date']})"
                            else:
                                item['reason'] = f"å³å°†åˆ°æœŸ (è¿˜å‰© {days_diff} å¤©)"
                            stale_items.append(item)
                    except:
                        pass

                return stale_items
        except Exception as e:
            print(f"Get Stale Items Error: {e}")
            return []

    @staticmethod
    def generate_chaser_message(item_details):
        """ç”Ÿæˆæ”¯æŒå¤šé£æ ¼çš„å‚¬å•/æé†’è¯æœ¯"""
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ICUåŒ»ç–—ä¿¡æ¯åŒ–é¡¹ç›®æ²Ÿé€šä¸“å®¶ï¼Œæ“…é•¿å†™å‚¬åŠæé†’ã€‚
ä½ å¿…é¡»åªè¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚

ä¸‰ç§é£æ ¼å¿…é¡»æœ‰æ˜æ˜¾å·®å¼‚ï¼š
- professional: åƒé¡¹ç›®ç»ç†å‘ç»™ç”²æ–¹çš„æ­£å¼é‚®ä»¶ã€‚ç”¨æ•°æ®è¯´è¯ï¼ˆé€¾æœŸå¤©æ•°ã€å½±å“èŒƒå›´ï¼‰ï¼Œè¯­æ°”ä¸¥è°¨å…‹åˆ¶ï¼Œèšç„¦è§£å†³æ–¹æ¡ˆã€‚ä¸è¯´"æ‚¨å¥½"å¼€å¤´ã€‚
- soft: åƒåŒäº‹é—´çš„å¾®ä¿¡æ¶ˆæ¯ã€‚å…ˆè®¤å¯å¯¹æ–¹çš„å·¥ä½œï¼Œå†è‡ªç„¶å¼•å‡ºè¿™ä¸ªäº‹é¡¹ï¼Œè¯­æ°”è½»æ¾ä½†ä¸å¤±åˆ†å¯¸ã€‚å¯ä»¥ç”¨"ï½""ğŸ˜Š"ç­‰ã€‚
- direct: åƒé¢†å¯¼çš„ç´§æ€¥é€šçŸ¥ã€‚å¼€é—¨è§å±±ï¼ŒæŒ‡å‡ºé£é™©åæœï¼Œæ˜ç¡®deadlineï¼Œè¯­æ°”æœæ–­æœ‰å‹è¿«æ„Ÿã€‚ç”¨"âš ï¸""â€¼ï¸"ç­‰å¼ºè°ƒã€‚

æ¯æ¡content 150-250å­—ã€‚subjectè¦ç®€æ´æœ‰åŠ›ï¼Œä¸è¶…è¿‡25å­—ã€‚
ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦ä»£ç å—ï¼š{"professional":{"subject":"æ ‡é¢˜","content":"æ­£æ–‡"},"soft":{"subject":"æ ‡é¢˜","content":"æ­£æ–‡"},"direct":{"subject":"æ ‡é¢˜","content":"æ­£æ–‡"}}"""

        import json
        title = item_details.get('title', 'æœªçŸ¥äº‹é¡¹')
        reason = item_details.get('reason', 'éœ€è¦è·Ÿè¿›')
        severity = item_details.get('severity', 'ä¸­')
        item_type = item_details.get('type', 'issue')
        
        # æå–é€¾æœŸå¤©æ•°
        delay_info = ""
        if 'è¶…è¿‡' in reason and 'å¤©' in reason:
            delay_info = reason
        
        try:
            user_content = f"""è¯·ä¸ºä»¥ä¸‹æ»åäº‹é¡¹ç”Ÿæˆä¸‰ç§é£æ ¼çš„å‚¬å•è¯æœ¯ï¼Œåªè¾“å‡ºJSONã€‚

äº‹é¡¹åç§°ï¼š{title}
ä¸¥é‡ç¨‹åº¦ï¼š{severity}
æ»åæƒ…å†µï¼š{reason}
äº‹é¡¹ç±»å‹ï¼š{item_type}

è¦æ±‚ï¼šä¸‰ç§é£æ ¼å¿…é¡»è¯­æ°”ã€æªè¾ã€ç»“æ„éƒ½æ˜æ˜¾ä¸åŒã€‚professionalè¦å¼•ç”¨å…·ä½“æ•°æ®ï¼Œsoftè¦æœ‰äººæƒ…å‘³ï¼Œdirectè¦æœ‰ç´§è¿«æ„Ÿã€‚"""
            print(f"[DEBUG] Chaser - calling AI with 90s timeout...")
            
            # ç”¨çº¿ç¨‹è¶…æ—¶åŒ…è£¹, é˜²æ­¢æµå¼è¯·æ±‚æ— é™é˜»å¡
            from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(ai_service.call_ai_api, system_prompt, user_content, "analysis")
                try:
                    ai_resp = future.result(timeout=90)
                except FuturesTimeoutError:
                    print("[DEBUG] Chaser - AI call timed out after 90s")
                    raise ValueError("AI è°ƒç”¨è¶…æ—¶(90s)")
            
            print(f"[DEBUG] Chaser - AI response length: {len(ai_resp) if ai_resp else 'None'}")
            print(f"[DEBUG] Chaser - AI response preview: {(ai_resp or '')[:200]}")
            
            if not ai_resp:
                raise ValueError("AI è¿”å›ä¸ºç©º")
            
            # æ¸…æ´—å¹¶æå– JSON
            cleaned = ai_resp.strip()
            # å»æ‰ markdown ä»£ç å—
            if '```' in cleaned:
                parts = cleaned.split('```')
                for part in parts[1:]:
                    p = part.strip()
                    if p.startswith('json'):
                        p = p[4:].strip()
                    if p.startswith('{'):
                        cleaned = p
                        break
            
            # æå– JSON å¯¹è±¡
            start = cleaned.find('{')
            end = cleaned.rfind('}')
            if start != -1 and end != -1:
                cleaned = cleaned[start:end+1]
                result = json.loads(cleaned)
                if 'professional' in result:
                    print(f"[DEBUG] Chaser - parsed OK, keys: {list(result.keys())}")
                    return result
            
            # JSON è§£æå¤±è´¥ï¼Œç”¨ AI çš„æ–‡æœ¬å†…å®¹æ„é€ ç»“æ„åŒ–ç»“æœ
            print(f"[DEBUG] Chaser - AI æœªè¿”å›JSONï¼Œç”¨åŸå§‹æ–‡æœ¬æ„é€ ç»“æœ")
            return {
                "professional": {
                    "subject": f"å…³äºã€Œ{title}ã€çš„è¿›åº¦åŒæ­¥",
                    "content": ai_resp[:150] if ai_resp else f"è¯·å…³æ³¨ã€Œ{title}ã€ï¼Œ{reason}ã€‚"
                },
                "soft": {
                    "subject": f"æ¸©é¦¨æé†’ï¼š{title}",
                    "content": f"è¾›è‹¦äº†ï¼æƒ³è·Ÿæ‚¨åŒæ­¥ä¸€ä¸‹ã€Œ{title}ã€çš„æœ€æ–°æƒ…å†µã€‚ç›®å‰{reason}ï¼Œæ–¹ä¾¿çš„è¯å¸®å¿™çœ‹çœ‹è¿›å±•å¦‚ä½•ï¼Ÿæœ‰ä»»ä½•éœ€è¦æ”¯æŒçš„åœ°æ–¹éšæ—¶è¯´ï½"
                },
                "direct": {
                    "subject": f"ã€ç´§æ€¥ã€‘{title} éœ€ç«‹å³å¤„ç†",
                    "content": f"ã€Œ{title}ã€{reason}ï¼Œå·²å½±å“é¡¹ç›®æ•´ä½“è¿›åº¦ã€‚è¯·ä»Šæ—¥å†…åé¦ˆå¤„ç†æ–¹æ¡ˆï¼Œå¦‚éœ€åè°ƒèµ„æºè¯·ç«‹å³å‘ŠçŸ¥ã€‚"
                }
            }
        except Exception as e:
            logger.error(f"Generate Chaser Error: {e}")
            return {
                "professional": {
                    "subject": f"å…³äºã€Œ{title}ã€çš„è¿›åº¦åŒæ­¥",
                    "content": f"æ‚¨å¥½ï¼Œå…³äºã€Œ{title}ã€äº‹é¡¹ï¼Œ{reason}ï¼Œè¯·åè°ƒç›¸å…³èµ„æºå°½å¿«æ¨è¿›ã€‚å¦‚æœ‰å›°éš¾è¯·åŠæ—¶åé¦ˆï¼Œæˆ‘ä»¬å…±åŒåˆ¶å®šè§£å†³æ–¹æ¡ˆã€‚"
                },
                "soft": {
                    "subject": f"æ¸©é¦¨æé†’ï¼š{title}",
                    "content": f"è¾›è‹¦äº†ï¼æƒ³è·Ÿæ‚¨åŒæ­¥ä¸€ä¸‹ã€Œ{title}ã€çš„æœ€æ–°æƒ…å†µã€‚ç›®å‰{reason}ï¼Œæ–¹ä¾¿çš„è¯å¸®å¿™çœ‹çœ‹è¿›å±•å¦‚ä½•ï¼Ÿæœ‰ä»»ä½•éœ€è¦æ”¯æŒçš„åœ°æ–¹éšæ—¶è¯´ï½"
                },
                "direct": {
                    "subject": f"ã€ç´§æ€¥ã€‘{title} éœ€ç«‹å³å¤„ç†",
                    "content": f"ã€Œ{title}ã€{reason}ï¼Œå·²å½±å“é¡¹ç›®æ•´ä½“è¿›åº¦ã€‚è¯·ä»Šæ—¥å†…åé¦ˆå¤„ç†æ–¹æ¡ˆï¼Œå¦‚éœ€åè°ƒèµ„æºè¯·ç«‹å³å‘ŠçŸ¥ã€‚"
                }
            }

    @staticmethod
    def auto_extract_knowledge(issue_id):
        """ä»å·²è§£å†³çš„é—®é¢˜ä¸­è‡ªåŠ¨æå–çŸ¥è¯†åº“æ¡ç›®"""
        try:
            with DatabasePool.get_connection() as conn:
                # 1. è·å–é—®é¢˜è¯¦æƒ…
                issue = conn.execute('SELECT * FROM issues WHERE id = ?', (issue_id,)).fetchone()
                if not issue:
                    return {"success": False, "message": "Issue not found"}
                
                # 2. è·å–ç›¸å…³çš„å·¥ä½œæ—¥å¿— (å°è¯•å…³è”)
                # ç®€å•æ¨¡ç³ŠåŒ¹é…ï¼šæ—¥å¿—å†…å®¹åŒ…å«é—®é¢˜æè¿°çš„ä¸€éƒ¨åˆ†? æˆ–è€…æš‚ä¸å…³è”ï¼Œä»…å‡­é—®é¢˜æè¿°åˆ†æ
                # ä¸ºäº†æ•ˆæœæ›´å¥½ï¼Œæˆ‘ä»¬å‡è®¾ AI èƒ½ä»é—®é¢˜æè¿°å’Œç±»å‹ä¸­æå–é€šç”¨ç»éªŒ
                
                # 3. è°ƒç”¨ AI æå–
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„é¡¹ç›®ç»ç†å’ŒçŸ¥è¯†ç®¡ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»å…·ä½“çš„é¡¹ç›®é—®é¢˜ä¸­æå–é€šç”¨çš„ã€å¯å¤ç”¨çš„â€œå¡«å‘æŒ‡å—â€æˆ–â€œæœ€ä½³å®è·µâ€ã€‚
è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. æå–å‡ºçš„çŸ¥è¯†åº”å½“å»é™¤å…·ä½“çš„é¡¹ç›®ç‰¹æŒ‡ä¿¡æ¯ï¼ˆå¦‚æŸæŸåŒ»é™¢ã€æŸæŸäººåï¼‰ï¼Œä½¿å…¶å…·æœ‰æ™®é€‚æ€§ã€‚
2. æ ¼å¼è¦æ±‚è¿”å› JSON: {"title": "çŸ¥è¯†æ¡ç›®æ ‡é¢˜", "content": "è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆæˆ–é¿å‘æŒ‡å—", "tags": "æ ‡ç­¾1,æ ‡ç­¾2"}
3. æ ‡é¢˜è¦è¨€ç®€æ„èµ…ï¼Œä¾‹å¦‚â€œOracleæ•°æ®åº“è¿æ¥è¶…æ—¶æ’æŸ¥æŒ‡å—â€ã€‚
4. å†…å®¹è¦åŒ…å«ï¼šé—®é¢˜ç°è±¡ã€æ ¹æœ¬åŸå› ï¼ˆæ¨æµ‹ï¼‰ã€è§£å†³æ–¹æ¡ˆ/å»ºè®®ã€‚
"""
                user_content = f"""
é—®é¢˜ç±»å‹: {issue['issue_type']}
ä¸¥é‡ç¨‹åº¦: {issue['severity']}
é—®é¢˜æè¿°: {issue['description']}
åˆ›å»ºæ—¶é—´: {issue['created_at']}
è§£å†³æ—¶é—´: {issue['resolved_at']}
"""
                ai_resp = ai_service.call_ai_api(system_prompt, user_content, task_type="json")
                
                # Clean up JSON
                if ai_resp.startswith('```json'):
                    ai_resp = ai_resp.replace('```json', '').replace('```', '')
                
                kb_data = json.loads(ai_resp)
                
                # 4. å­˜å…¥ Knowledge Base
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç±»ä¼¼æ ‡é¢˜ (ç®€å•å»é‡)
                existing = conn.execute('SELECT id FROM knowledge_base WHERE title = ?', (kb_data['title'],)).fetchone()
                if existing:
                    return {"success": True, "message": "Knowledge item already exists", "id": existing['id']}
                
                cursor = conn.execute('''
                    INSERT INTO knowledge_base (title, content, category, tags, created_at)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    kb_data['title'], 
                    kb_data['content'], 
                    'AI æç‚¼', 
                    kb_data.get('tags', 'è‡ªåŠ¨æå–'), 
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                ))
                new_id = cursor.lastrowid
                return {"success": True, "message": "Extracted successfully", "id": new_id, "data": kb_data}
                
        except Exception as e:
            print(f"Auto Extract Knowledge Error: {e}")
            return {"success": False, "message": str(e)}

    @staticmethod
    def find_similar_projects(project_id):
        """æŸ¥æ‰¾ç›¸ä¼¼é¡¹ç›® (è§„åˆ™åˆç­› + AI æ’åº)"""
        try:
            with DatabasePool.get_connection() as conn:
                target = conn.execute('SELECT * FROM projects WHERE id = ?', (project_id,)).fetchone()
                if not target:
                    return []
                
                # 1. ç²—ç­› (è·å–æ‰€æœ‰å…¶ä»–é¡¹ç›®)
                all_projects = conn.execute('SELECT id, project_name, hospital_name, status, risk_score FROM projects WHERE id != ?', (project_id,)).fetchall()
                
                candidates = []
                common_terms = ['ICU', 'é‡ç—‡', 'éº»é†‰', 'æ‰‹æœ¯', 'æ€¥è¯Š', 'æŠ¤ç†', 'é›†æˆ']
                
                for p in all_projects:
                    score = 0
                    
                    # åŒä¸€åŒ»é™¢
                    if p['hospital_name'] == target['hospital_name']:
                        score += 50
                    
                    # åç§°å…³é”®è¯åŒ¹é…
                    target_name = target['project_name'] or ''
                    p_name = p['project_name'] or ''
                    for term in common_terms:
                        if term in target_name and term in p_name:
                            score += 20
                    
                    # åªè¦æœ‰ä¸€ç‚¹ç›¸ä¼¼åº¦å°±çº³å…¥å€™é€‰
                    if score > 0:
                        candidates.append(dict(p))
                
                # æŒ‰åˆ†æ•°æ’åºå–å‰ 10
                candidates.sort(key=lambda x: x.get('score', 0), reverse=True)
                top_10 = candidates[:10]
                
                if not top_10:
                    return []
                
                # 2. AI æ·±åº¦åˆ†æä¸æ’åº
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªé¡¹ç›®ç»„åˆç®¡ç†ä¸“å®¶ã€‚è¯·æ ¹æ®ç›®æ ‡é¡¹ç›®å’Œå€™é€‰é¡¹ç›®åˆ—è¡¨ï¼Œæ‰¾å‡ºæœ€ç›¸ä¼¼çš„ 3 ä¸ªé¡¹ç›®ã€‚
å…³æ³¨ç‚¹ï¼šä¸šåŠ¡é¢†åŸŸï¼ˆå¦‚éƒ½æ˜¯ICUï¼‰ã€åŒ»é™¢èƒŒæ™¯ï¼ˆå¦‚åŒä¸€å®¶åŒ»é™¢ï¼‰ã€é¡¹ç›®é˜¶æ®µæˆ–é£é™©çŠ¶å†µã€‚
è¿”å› JSON æ•°ç»„: [{"id": å€™é€‰é¡¹ç›®ID, "reason": "ç›¸ä¼¼åŸå› ç®€è¿° (15å­—ä»¥å†…)"}]"""

                candidate_json = json.dumps(
                    [{k: v for k, v in c.items() if k in ['id', 'project_name', 'hospital_name', 'status', 'risk_score']} for c in top_10],
                    ensure_ascii=False
                )
                user_content = f"""ä½ æ˜¯ä¸€ä¸ªé¡¹ç›®ç»„åˆç®¡ç†ä¸“å®¶ã€‚è¯·æ ¹æ®ç›®æ ‡é¡¹ç›®å’Œå€™é€‰é¡¹ç›®åˆ—è¡¨ï¼Œæ‰¾å‡ºæœ€ç›¸ä¼¼çš„ 3 ä¸ªé¡¹ç›®ã€‚
å…³æ³¨ç‚¹ï¼šä¸šåŠ¡é¢†åŸŸï¼ˆå¦‚éƒ½æ˜¯ICUï¼‰ã€åŒ»é™¢èƒŒæ™¯ï¼ˆå¦‚åŒä¸€å®¶åŒ»é™¢ï¼‰ã€é¡¹ç›®é˜¶æ®µæˆ–é£é™©çŠ¶å†µã€‚
ä¸¥ç¦è‡ªæˆ‘ä»‹ç»æˆ–å®¢å¥—ï¼Œç›´æ¥è¿”å›JSONæ•°ç»„ã€‚

ç›®æ ‡é¡¹ç›®: {target['project_name']} (åŒ»é™¢: {target['hospital_name']}, çŠ¶æ€: {target['status']}, é£é™©åˆ†: {target['risk_score']})

å€™é€‰åˆ—è¡¨:
{candidate_json}

è¯·ç›´æ¥è¾“å‡ºJSONæ•°ç»„ï¼ˆä¸è¦ä»£ç å—æ ‡è®°ï¼‰: [{{"id": å€™é€‰é¡¹ç›®ID, "reason": "ç›¸ä¼¼åŸå› ç®€è¿°(15å­—ä»¥å†…)"}}]"""

                ai_resp = ai_service.call_ai_api(system_prompt, user_content, task_type="json")
                
                if not ai_resp:
                    print("[DEBUG] find_similar_projects: AI returned None")
                    return top_10[:3]  # AI å¤±è´¥æ—¶ç›´æ¥è¿”å›ç²—ç­›å‰3
                
                # æ¸…æ´— JSON
                cleaned = ai_resp.strip()
                if '```' in cleaned:
                    parts = cleaned.split('```')
                    for part in parts[1:]:
                        p = part.strip()
                        if p.startswith('json'):
                            p = p[4:].strip()
                        if p.startswith('['):
                            cleaned = p
                            break
                
                start = cleaned.find('[')
                end = cleaned.rfind(']')
                if start != -1 and end != -1:
                    cleaned = cleaned[start:end+1]
                
                ranked_results = json.loads(cleaned)
                
                # å›å¡«è¯¦ç»†ä¿¡æ¯
                final_projects = []
                candidate_map = {c['id']: c for c in top_10}
                
                for res in ranked_results:
                    pid = res.get('id')
                    if pid in candidate_map:
                        proj = candidate_map[pid]
                        proj['similarity_reason'] = res.get('reason', 'ç›¸ä¼¼é¡¹ç›®')
                        final_projects.append(proj)
                        
                return final_projects

        except Exception as e:
            print(f"Find Similar Projects Error: {e}")
            return []

    @staticmethod
    def detect_anomalies(project_id):
        """æ£€æµ‹é¡¹ç›®å¼‚å¸¸ï¼šé™é»˜ã€åœæ»ã€é—®é¢˜çªå¢"""
        anomalies = []
        try:
            with DatabasePool.get_connection() as conn:
                # 1. é™é»˜æ£€æµ‹ (Silence): è¶…è¿‡3ä¸ªå·¥ä½œæ—¥(ç®€å•æŒ‰4å¤©)æ— æ—¥æŠ¥
                last_log = conn.execute('SELECT MAX(log_date) as last_date FROM work_logs WHERE project_id = ?', (project_id,)).fetchone()
                if not last_log['last_date']:
                    # ä»æœªå†™è¿‡æ—¥å¿—?
                    pass
                else:
                    last_date = datetime.strptime(last_log['last_date'], '%Y-%m-%d')
                    if datetime.now() - last_date > timedelta(days=4):
                        anomalies.append({
                            "type": "anomaly",
                            "priority": "High",
                            "title": "é¡¹ç›®é™é»˜é¢„è­¦",
                            "description": f"å·²æœ‰ {(datetime.now() - last_date).days} å¤©æœªæäº¤å·¥ä½œæ—¥æŠ¥ã€‚",
                            "suggestion": "è¯·ç¡®è®¤å›¢é˜Ÿæ˜¯å¦åœ¨æ­£å¸¸æ¨è¿›ï¼Œæˆ–æé†’è¡¥å½•æ—¥å¿—ã€‚",
                            "action_label": "æé†’æ—¥å¿—",
                            "action_tab": "worklogs"
                        })

                # 2. åœæ»æ£€æµ‹ (Stagnation): è¿›åº¦è¿ç»­14å¤©æ— å˜åŒ– (ä¸”çŠ¶æ€ä¸ºè¿›è¡Œä¸­)
                project = conn.execute('SELECT status, progress FROM projects WHERE id = ?', (project_id,)).fetchone()
                if project and project['status'] == 'è¿›è¡Œä¸­':
                    # è·å–æœ€è¿‘çš„ä¸€æ¡è¿›åº¦è®°å½•
                    last_history = conn.execute('''
                        SELECT progress, record_date FROM progress_history 
                        WHERE project_id = ? 
                        ORDER BY record_date DESC LIMIT 1
                    ''', (project_id,)).fetchone()
                    
                    if last_history:
                        last_date = datetime.strptime(last_history['record_date'], '%Y-%m-%d')
                        days_diff = (datetime.now() - last_date).days
                        
                        # å¦‚æœæœ€è¿‘ä¸€æ¬¡è®°å½•è¶…è¿‡14å¤©ï¼Œä¸”è¿›åº¦ä¸å½“å‰ä¸€è‡´ (è¯´æ˜è¿™14å¤©æ²¡å˜è¿‡)
                        if days_diff > 14 and last_history['progress'] == project['progress']:
                            anomalies.append({
                                "type": "anomaly",
                                "priority": "High",
                                "title": "è¿›åº¦åœæ»é¢„è­¦",
                                "description": f"é¡¹ç›®è¿›åº¦å·²è¿ç»­ {days_diff} å¤©åœç•™åœ¨ {project['progress']}%ã€‚",
                                "suggestion": "é¡¹ç›®å¯èƒ½å—é˜»ï¼Œå»ºè®®å®¡æŸ¥é˜»å¡åŸå› æˆ–è°ƒæ•´è®¡åˆ’ã€‚",
                                "action_label": "åˆ†æåå·®",
                                "action_tab": "deviation"
                            })

                # 3. é—®é¢˜çªå¢ (Issue Spike): è¿‘3å¤©æ–°å¢é—®é¢˜æ•° > è¿‡å»30å¤©åŠå‡å€¼ * 2
                three_days_ago = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
                thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
                
                recent_count = conn.execute('SELECT COUNT(*) as c FROM issues WHERE project_id=? AND created_at >= ?', (project_id, three_days_ago)).fetchone()['c']
                past_count = conn.execute('SELECT COUNT(*) as c FROM issues WHERE project_id=? AND created_at >= ? AND created_at < ?', (project_id, thirty_days_ago, three_days_ago)).fetchone()['c']
                
                if past_count > 0:
                    avg_3days = (past_count / 27.0) * 3 # è¿‡å»27å¤©çš„æ¯3å¤©å¹³å‡
                    if recent_count > max(5, avg_3days * 2): # é˜ˆå€¼: è‡³å°‘5ä¸ªä¸”æ˜¯å¹³å‡çš„2å€
                        anomalies.append({
                            "type": "anomaly",
                            "priority": "High",
                            "title": "é—®é¢˜æ¿€å¢é¢„è­¦",
                            "description": f"è¿‘3å¤©æ–°å¢ {recent_count} ä¸ªé—®é¢˜ï¼Œè¿œè¶…å¹³æ—¶æ°´å¹³ã€‚",
                            "suggestion": "å¯èƒ½å¤„äºæµ‹è¯•çˆ†å‘æœŸæˆ–è´¨é‡å¤±æ§ï¼Œå»ºè®®ä»‹å…¥è´¨é‡è¯„ä¼°ã€‚",
                            "action_label": "æŸ¥çœ‹é—®é¢˜",
                            "action_tab": "issues"
                        })

                # 4. çŠ¶æ€å€’é€€ (Status Reversal): è¿™é‡Œæ¼”ç¤ºæ£€æµ‹å·²è§£å†³é—®é¢˜è¢«é‡æ–°æ‰“å¼€
                reopened_issues = conn.execute('''
                    SELECT COUNT(*) as c FROM issues 
                    WHERE project_id = ? AND status != 'å·²è§£å†³' 
                    AND description LIKE '%è¢«é‡æ–°æ‰“å¼€%' 
                ''', (project_id,)).fetchone()['c']
                
                if reopened_issues > 0:
                    anomalies.append({
                        "type": "reversal",
                        "priority": "High",
                        "title": "ä»»åŠ¡å€’é€€é¢„è­¦",
                        "description": f"æœ‰ {reopened_issues} ä¸ªå·²è§£å†³çš„é—®é¢˜è¢«é‡æ–°æ‰“å¼€ï¼Œå¯èƒ½å­˜åœ¨ä¿®å¤ä¸å½»åº•çš„æƒ…å†µã€‚",
                        "suggestion": "è¯·æŠ€æœ¯è´Ÿè´£äººä»‹å…¥ï¼Œå®¡æŸ¥é‡å¼€åŸå› ï¼Œé¿å…é‡å¤è¿”å·¥ã€‚",
                        "action_label": "å®¡æŸ¥é‡å¼€",
                        "action_tab": "issues"
                    })

        except Exception as e:
            print(f"Anomaly Detection Error: {e}")
        
        return anomalies

    @staticmethod
    def predict_future_risks(project_id):
        """é¢„æµ‹æ€§é£é™©åˆ†æï¼šé¢„æµ‹å»¶æœŸæ¦‚ç‡å’Œå®Œæˆæ—¥æœŸ (1-2å‘¨é¢„åˆ¤)"""
        try:
            with DatabasePool.get_connection() as conn:
                project = conn.execute('SELECT id, project_name, progress, plan_end_date, plan_start_date FROM projects WHERE id = ?', (project_id,)).fetchone()
                if not project: 
                    logger.warning(f"Project {project_id} not found for prediction")
                    return None
                
                # 1. è®¡ç®—äº¤ä»˜é€Ÿåº¦ (Velocity)
                # è·å–è¿‡å» 14 å¤©çš„è¿›åº¦å˜åŒ–
                fourteen_days_ago = (datetime.now() - timedelta(days=14)).strftime('%Y-%m-%d')
                history = conn.execute('''
                    SELECT progress, record_date FROM progress_history 
                    WHERE project_id = ? AND record_date >= ? 
                    ORDER BY record_date ASC
                ''', (project_id, fourteen_days_ago)).fetchall()
                
                velocity = 0 # æ¯å¤©å¹³å‡å¢é•¿ç™¾åˆ†æ¯”
                if len(history) >= 2:
                    start_p = history[0]['progress'] or 0
                    end_p = history[-1]['progress'] or 0
                    try:
                        days = (datetime.strptime(history[-1]['record_date'], '%Y-%m-%d') - datetime.strptime(history[0]['record_date'], '%Y-%m-%d')).days
                        if days > 0:
                            velocity = (end_p - start_p) / float(days)
                    except Exception as ex:
                        logger.error(f"Error calculating velocity days: {ex}")
                
                # 2. è·å–æƒ…ç»ªè¯„åˆ†è¶‹åŠ¿ (Sentiment Trend)
                risk_history = conn.execute('''
                    SELECT sentiment_score FROM project_risk_history 
                    WHERE project_id = ? 
                    ORDER BY record_date DESC LIMIT 5
                ''', (project_id,)).fetchall()
                
                # è¿‡æ»¤ None å€¼
                sentiment_scores = [r['sentiment_score'] for r in risk_history if r['sentiment_score'] is not None]
                sentiment_avg = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 50
                
                is_sentiment_dropping = False
                if len(risk_history) >= 2:
                    current_s = risk_history[0]['sentiment_score'] or 0
                    prev_s = risk_history[-1]['sentiment_score'] or 0
                    is_sentiment_dropping = current_s < prev_s # å› ä¸ºç¬¬0ä¸ªæ˜¯æœ€è¿‘çš„

                # 3. è®¡ç®—é¢„æµ‹æ—¥æœŸ
                current_progress = project['progress'] or 0
                remaining_p = 100 - current_progress
                if velocity > 0:
                    days_needed = remaining_p / velocity
                    # é™åˆ¶å¤©æ•°ï¼Œé˜²æ­¢æº¢å‡º
                    days_needed = min(days_needed, 3650) # æœ€å¤šé¢„æµ‹10å¹´
                    predicted_end = (datetime.now() + timedelta(days=int(days_needed))).strftime('%Y-%m-%d')
                else:
                    predicted_end = "æ— æ³•è®¡ç®— (è¿›åº¦åœæ»)"
                    days_needed = 999
                
                # 4. åˆ¤å®šå»¶æœŸé£é™©
                is_delay_predicted = False
                delay_days = 0
                if project['plan_end_date'] and velocity > 0:
                    try:
                        plan_end = datetime.strptime(project['plan_end_date'], '%Y-%m-%d')
                        actual_pred = datetime.now() + timedelta(days=int(days_needed))
                        if actual_pred > plan_end:
                            is_delay_predicted = True
                            delay_days = (actual_pred - plan_end).days
                    except Exception as pe:
                        logger.error(f"Error parsing plan_end_date for project {project_id}: {pe}")

                return {
                    "project_id": project_id,
                    "current_progress": current_progress,
                    "avg_velocity": round(velocity, 2),
                    "predicted_end_date": predicted_end,
                    "plan_end_date": project['plan_end_date'],
                    "is_delay_predicted": is_delay_predicted,
                    "delay_days": delay_days,
                    "sentiment_score": round(sentiment_avg, 1),
                    "is_sentiment_dropping": is_sentiment_dropping,
                    "risk_level": "High" if (is_delay_predicted and delay_days > 7) or is_sentiment_dropping else "Normal"
                }

        except Exception as e:
            logger.error(f"Predict Future Risks Error for project {project_id}: {e}", exc_info=True)
            return None

    @staticmethod
    def get_recommended_actions(project_id, force_refresh=False):
        """åŸºäºé£é™©ã€æ»åé¡¹ã€å¼‚åŠ¨å’Œè¿›åº¦ç”Ÿæˆå†³ç­–å»ºè®®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰"""
        try:
            today_date = datetime.now().strftime('%Y-%m-%d')
            
            # 1. æ£€æŸ¥ç¼“å­˜
            if not force_refresh:
                with DatabasePool.get_connection() as conn:
                    cache = conn.execute('''
                        SELECT content FROM ai_report_cache 
                        WHERE project_id = ? AND report_type = 'recommended_actions'
                        AND date(created_at) = ?
                    ''', (project_id, today_date)).fetchone()
                    if cache:
                        return json.loads(cache['content'])

            actions = []
            # 0. å¼‚å¸¸æ£€æµ‹ (ä¼˜å…ˆå±•ç¤º)
            anomalies = AIInsightService.detect_anomalies(project_id)
            actions.extend(anomalies)

            # 1. è·å–åŸºç¡€æ•°æ®
            stale_items = AIInsightService.get_stale_items(project_id)
            
            with DatabasePool.get_connection() as conn:
                # è·å–æœ€æ–°é£é™©è¯„åˆ†
                risk_record = conn.execute('''
                    SELECT risk_score, sentiment_score, key_risk_factors 
                    FROM project_risk_history 
                    WHERE project_id = ? 
                    ORDER BY record_date DESC LIMIT 1
                ''', (project_id,)).fetchone()
                
                # è·å–é¡¹ç›®åŸºæœ¬ä¿¡æ¯
                project = conn.execute('SELECT project_name, status, progress FROM projects WHERE id = ?', (project_id,)).fetchone()

            # 2. è§„åˆ™å¼•æ“ç”Ÿæˆå»ºè®®
            
            # (A) é£é™©å¹²é¢„
            current_risk = risk_record['risk_score'] if risk_record else 0
            if current_risk > 80:
                actions.append({
                    "type": "risk",
                    "priority": "High",
                    "title": "å¬å¼€é£é™©å¤ç›˜ä¼šè®®",
                    "description": f"å½“å‰é£é™©è¯„åˆ†é«˜è¾¾ {current_risk}ï¼Œä¸»è¦é£é™©å› ç´ : {risk_record['key_risk_factors'] or 'æœªçŸ¥'}ã€‚",
                    "suggestion": "å»ºè®®ç«‹å³ç»„ç»‡é¡¹ç›®ç»„+ç”²æ–¹å…³é”®å¹²ç³»äººè¿›è¡Œé£é™©å¯¹é½ã€‚",
                    "action_label": "æŸ¥çœ‹é£é™©è¯¦æƒ…",
                    "action_tab": "dashboard" # å¯¹åº”å‰ç«¯ Tab
                })
            elif current_risk > 60:
                actions.append({
                    "type": "risk",
                    "priority": "Medium",
                    "title": "å…³æ³¨é£é™©è¶‹åŠ¿",
                    "description": f"é¡¹ç›®å­˜åœ¨ä¸€å®šé£é™© (è¯„åˆ† {current_risk})ã€‚",
                    "suggestion": "å»ºè®®åœ¨å‘¨ä¼šä¸­é‡ç‚¹åŒæ­¥é£é™©æ¶ˆå‡è®¡åˆ’ã€‚",
                    "action_label": "æŸ¥çœ‹è¶‹åŠ¿",
                    "action_tab": "dashboard"
                })

            # (B) æ»åé¡¹æ¸…ç†
            stale_issues = [i for i in stale_items if i['type'] == 'issue']
            if len(stale_issues) > 3:
                actions.append({
                    "type": "issue",
                    "priority": "High",
                    "title": "æ¸…ç†ç§¯å‹é—®é¢˜",
                    "description": f"å‘ç° {len(stale_issues)} ä¸ªæ»åé—®é¢˜ï¼ˆè¶…è¿‡7å¤©æœªè§£å†³ï¼‰ã€‚",
                    "suggestion": "å»ºè®®å®‰æ’ä¸“é¡¹èµ„æºè¿›è¡Œæ”»åšï¼Œæˆ–é‡æ–°è¯„ä¼°é—®é¢˜ä¼˜å…ˆçº§ã€‚",
                    "action_label": "å¤„ç†é—®é¢˜",
                    "action_tab": "issues"
                })
            elif len(stale_issues) > 0:
                 actions.append({
                    "type": "issue",
                    "priority": "Medium",
                    "title": "è·Ÿè¿›æ»åé—®é¢˜",
                    "description": f"å­˜åœ¨ {len(stale_issues)} ä¸ªé•¿æœŸæœªè§£å†³çš„é—®é¢˜ã€‚",
                    "suggestion": "è¯·ç¡®è®¤æ˜¯å¦é˜»å¡é¡¹ç›®è¿›åº¦ï¼Œå¿…è¦æ—¶ä½¿ç”¨ AI å‚¬å•åŠŸèƒ½ã€‚",
                    "action_label": "AI å‚¬å•",
                    "action_tab": "issues" # Special handling in frontend to open modal?
                })

            # (C) é‡Œç¨‹ç¢‘ä¿éšœ
            stale_milestones = [i for i in stale_items if i['type'] == 'milestone']
            if stale_milestones:
                m = stale_milestones[0]
                actions.append({
                    "type": "milestone",
                    "priority": "High",
                    "title": f"ä¿éšœé‡Œç¨‹ç¢‘: {m['title']}",
                    "description": m['reason'],
                    "suggestion": "é‡Œç¨‹ç¢‘å»¶æœŸé£é™©è¾ƒé«˜ï¼Œå»ºè®®æ¯æ—¥åŒæ­¥è¿›åº¦å¹¶å‘ç”²æ–¹é€šæŠ¥ã€‚",
                    "action_label": "æŸ¥çœ‹é‡Œç¨‹ç¢‘",
                    "action_tab": "milestones"
                })

            # (D) è¿›åº¦åå·® (ç®€å•é€»è¾‘)
            if project and project['status'] == 'è¿›è¡Œä¸­' and (project['progress'] or 0) < 50:
                # å‡è®¾åº”è¯¥æ›´é«˜? è¿™é‡Œåªæ˜¯ç¤ºä¾‹è§„åˆ™
                pass

            # 3. (å¯é€‰) AI ç»¼åˆåˆ†æå¢å¼º
            # å¦‚æœè§„åˆ™ç”Ÿæˆçš„å»ºè®®è¾ƒå¤šï¼Œæˆ‘ä»¬è°ƒç”¨ AI è¿›è¡Œä¸€æ¬¡â€œå†³ç­–å‹ç¼©â€
            if len(actions) > 2:
                summary_prompt = """ä½ æ˜¯ä¸€åèµ„æ·± PMO ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¤šæ¡é›¶æ•£çš„å»ºè®®æç‚¼ä¸ºä¸€æ¡æœ€é‡è¦çš„â€œé¡¹ç›®ç»ç†å”¯ä¸€æ ¸å¿ƒä»»åŠ¡â€ã€‚
ç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸¥ç¦ä»»ä½•è§£é‡Šæˆ–å¼•å¯¼æ€§è¯­è¨€ã€‚15å­—ä»¥å†…ï¼ŒåŠ¨è¯å¼€å¤´ï¼Œæå…·å·å¬åŠ›ã€‚"""
                actions_summary = "\n".join([f"- {a['title']}: {a['description']}" for a in actions])
                refined_command = ai_service.call_ai_api(summary_prompt, actions_summary, task_type="analysis")
                if refined_command and len(refined_command) < 50:
                    actions.insert(0, {
                        "type": "ai_command",
                        "priority": "High",
                        "title": "âš¡ AI å†³ç­–æŒ‡ä»¤",
                        "description": refined_command.strip('\"'),
                        "suggestion": "è¿™æ˜¯åŸºäºå¤šé¡¹é£é™©å› ç´ æç‚¼çš„æ ¸å¿ƒæŒ‡ä»¤ã€‚",
                        "action_label": "ç«‹å³å¤„ç†",
                        "action_tab": "dashboard"
                    })
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            priority_map = {"High": 0, "Medium": 1, "Low": 2}
            actions.sort(key=lambda x: priority_map.get(x['priority'], 99))

            # 4. æ›´æ–°ç¼“å­˜
            with DatabasePool.get_connection() as conn:
                conn.execute('''
                    DELETE FROM ai_report_cache 
                    WHERE project_id = ? AND report_type = 'recommended_actions'
                    AND date(created_at) = ?
                ''', (project_id, today_date))
                
                conn.execute('''
                    INSERT INTO ai_report_cache (project_id, report_type, content, created_at)
                    VALUES (?, ?, ?, ?)
                ''', (project_id, 'recommended_actions', json.dumps(actions, ensure_ascii=False), datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
                conn.commit()

            return actions

        except Exception as e:
            print(f"Get Recommended Actions Error: {e}")
            return []

    @staticmethod
    def analyze_demand_change(project_id, description):
        """
        åˆ†æéœ€æ±‚å˜æ›´çš„å½±å“ï¼šè´è¶æ•ˆåº”ã€å»¶æœŸæ¦‚ç‡ã€èµ„æºæˆæœ¬ã€‚
        """
        try:
            # 1. è·å–é¡¹ç›®åŸºæœ¬å¿«ç…§
            with DatabasePool.get_connection() as conn:
                project = conn.execute('SELECT * FROM projects WHERE id = ?', (project_id,)).fetchone()
                tasks = conn.execute('''
                    SELECT t.* FROM tasks t
                    JOIN project_stages s ON t.stage_id = s.id
                    WHERE s.project_id = ? AND t.is_completed = 0
                ''', (project_id,)).fetchall()

                milestones = conn.execute('SELECT * FROM milestones WHERE project_id = ? AND is_completed = 0', (project_id,)).fetchall()

            # 2. è°ƒç”¨ AI è¿›è¡Œå¤šç»´è¯„ä¼°
            prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„äº¤ä»˜æ€»ç›‘å’Œ PMO ä¸“å®¶ã€‚ç°æœ‰é¡¹ç›®â€œ{project['project_name']}â€é¢ä¸´ä¸€é¡¹éœ€æ±‚å˜æ›´ã€‚
            
            å˜æ›´æè¿°ï¼š
            {description}
            
            é¡¹ç›®ç°çŠ¶ï¼š
            - å½“å‰è¿›åº¦ï¼š{project['progress']}%
            - å¾…åŠä»»åŠ¡æ•°ï¼š{len(tasks)}
            - å¾…è¾¾æˆé‡Œç¨‹ç¢‘ï¼š{len(milestones)}
            
            è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±åº¦è¯„ä¼°å¹¶ç»™å‡ºç»“æ„åŒ–çš„ Markdown æŠ¥å‘Šï¼š
            1. **æ ¸å¿ƒå½±å“ (Core Impact)**ï¼šå¯¹ç°æœ‰æ¶æ„å’Œäº¤ä»˜è¿›åº¦çš„ç›´æ¥å†²å‡»ã€‚
            2. **è´è¶æ•ˆåº” (Ripple Effect)**ï¼šè¯¥å˜æ›´å¯èƒ½å¼•å‘çš„å…¶ä»–æ¨¡å—é£é™©æˆ–ååŒéƒ¨é—¨å‹åŠ›ã€‚
            3. **å»¶æœŸé£é™©è¯„ä¼°**ï¼šæ ¹æ®å˜æ›´å¤æ‚åº¦é¢„æµ‹å¯èƒ½çš„å·¥æœŸåå·®ï¼ˆä»¥å¤©ä¸ºå•ä½ï¼‰ã€‚
            4. **èµ„æº/æˆæœ¬è¯„ä¼°**ï¼šæ˜¯å¦éœ€è¦è¿½åŠ äººåŠ›æˆ–ç¡¬ä»¶æŠ•å…¥ã€‚
            5. **å†³ç­–å»ºè®®**ï¼šæ¥å—è¯¥å˜æ›´çš„æ¡ä»¶å»ºè®®ï¼ˆå¦‚å‹ç¼©éæ ¸å¿ƒä»»åŠ¡ã€ç”³è¯·å»¶æœŸç­‰ï¼‰ã€‚
            
            è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•å¼€åœºç™½ã€‚
            """
            
            # Call AI with correct method
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„äº¤ä»˜æ€»ç›‘å’Œ PMO ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„éœ€æ±‚å˜æ›´æè¿°å’Œé¡¹ç›®ç°çŠ¶ï¼Œè¿›è¡Œæ·±åº¦è¯„ä¼°ã€‚"
            user_content = prompt 
            
            analysis = ai_service.call_ai_api(system_prompt, user_content, task_type="analysis")
            return analysis

        except Exception as e:
            print(f"Analyze Demand Change Error: {e}")
            return "è¯„ä¼°ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚"


    @staticmethod
    def parse_multi_logs(raw_text):
        """è§£ææ‰¹é‡æ—¥å¿—æ–‡æœ¬"""
        # 1. å°è¯•ä½¿ç”¨ AI æ‰¹é‡è§£æ
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ—¥å¿—æ•´ç†åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šè¾“å…¥ä¸€æ®µåŒ…å«å¤šäººã€å¤šæ¡å·¥ä½œå†…å®¹çš„æ··åˆæ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯èŠå¤©è®°å½•æˆ–å‘¨æŠ¥ï¼‰ã€‚
è¯·å°†å…¶æ‹†è§£ä¸ºæ ‡å‡†çš„å·¥ä½œæ—¥å¿—åˆ—è¡¨ã€‚
è¿”å› JSON æ•°ç»„: [{"member_name": "å§“å", "log_date": "YYYY-MM-DD", "work_content": "å†…å®¹", "work_hours": 8.0, "issues": "æ— ", "plan": "æ˜æ—¥è®¡åˆ’"}]
è§„åˆ™ï¼š
1. è‡ªåŠ¨è¯†åˆ«æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©ã€‚
2. è‡ªåŠ¨è¯†åˆ«å§“åï¼Œå¦‚æœæœªæåŠï¼Œæ ‡è®°ä¸º"æœªçŸ¥"ã€‚
3. æå–å·¥æ—¶ï¼Œé»˜è®¤8å°æ—¶ã€‚
"""
        import json
        try:
            ai_resp = ai_service.call_ai_api(system_prompt, raw_text, task_type="json")
            if ai_resp.startswith('```json'):
                ai_resp = ai_resp.replace('```json', '').replace('```', '')
            
            logs = json.loads(ai_resp)
            if isinstance(logs, list):
                return logs
            return []
        except Exception as e:
            print(f"Parse Multi Logs Error: {e}")
            return []

ai_insight_service = AIInsightService()
